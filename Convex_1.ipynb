{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c440ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 101)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = np.loadtxt(\"samples.csv\", delimiter=\",\")\n",
    "print(samples.shape)\n",
    "# 1000 samples with 100 features each and a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053fc968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status: optimal_inaccurate\n",
      "optimal value 2.906898597653035e-06\n",
      "optimal var [ 5.06588574e-02 -5.95797114e-02  1.82092115e+00 -2.44486554e-01\n",
      " -2.36757664e+00  8.38680696e-02  6.47585633e-01 -1.16286388e+00\n",
      " -3.70768858e-01 -2.83654378e-01 -9.72149296e-01 -2.17355877e+00\n",
      " -1.45820006e+00  5.37591065e-01 -1.14187019e-02  1.68736075e+00\n",
      " -5.84488898e-01 -2.01471451e-02 -9.00750146e-01  6.54764665e-02\n",
      " -1.96524557e+00  2.07060530e+00 -1.81903029e+00 -9.00161894e-01\n",
      " -2.43145262e+00  2.11478967e+00  1.11655441e-01 -9.57377106e-01\n",
      "  1.76654277e+00 -2.63104979e+00  1.16186782e-02  6.88893336e-02\n",
      "  1.51212849e-01  1.30492858e+00 -5.18994707e-01 -1.62048276e+00\n",
      " -2.61645815e-01 -2.53919348e+00  3.47105572e-01 -1.95835638e+00\n",
      "  1.18701740e+00  6.86506249e-01  4.10215960e-01 -1.58815664e+00\n",
      " -1.75610835e+00  2.10212552e+00 -2.01297025e+00 -2.50256717e+00\n",
      " -1.94086419e+00  1.15247817e+00 -2.76731487e-01 -2.94485828e-01\n",
      "  4.98048361e-02 -1.45990055e+00 -1.44015739e+00 -1.00856561e+00\n",
      " -1.38490363e+00  9.64193210e+01 -2.76221748e+00  1.08889409e+00\n",
      " -2.32375931e+00  8.56542837e-01  1.19383320e-01  5.70399138e-01\n",
      "  1.42751419e+00  2.08805404e+00  2.87321379e-01 -1.66688856e+00\n",
      "  8.84449221e-01 -2.97877143e+00  2.09425866e+00  3.81356867e-01\n",
      " -1.25091275e+00  2.44189300e+00  6.39681086e-01 -1.64875931e-01\n",
      " -5.39445518e-01  5.02664987e+00  1.51031214e+00  1.08716905e+00\n",
      " -3.07965758e+00 -2.29658344e+00 -2.32068593e+00 -9.56545226e-01\n",
      " -1.88822304e+00  5.99953147e-01 -1.63883915e-01  9.40801084e-01\n",
      "  1.64579934e+00 -3.23212371e+00  1.69857372e+00 -2.31239925e+00\n",
      "  1.92769514e+00  2.15491077e-01 -5.29258651e-01  1.21997206e+00\n",
      "  4.64478907e+00 -4.59037489e-01  4.54471241e-02  8.85554472e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomasmatteson/miniconda3/envs/jup-env/lib/python3.9/site-packages/cvxpy/problems/problem.py:1296: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "x = samples[:, :-1]\n",
    "y = samples[:, -1]\n",
    "#print(x[1], y[1])\n",
    "#print(np.exp(x))\n",
    "#raise error\n",
    "# Create two scalar optimization variables.\n",
    "w = cp.Variable(x.shape[1])\n",
    "# no constraints\n",
    "# (1/x.shape[0])*\n",
    "# Form objective.\n",
    "log = cp.atoms.elementwise.logistic.logistic(cp.multiply(-y, x@w))\n",
    "obj = cp.Minimize((1/x.shape[0])*cp.sum(log))\n",
    "#obj = cp.Minimize((1/x.shape[0])*np.sum([np.log(1+np.exp(-y[i]*np.dot(w, x[i]))) for i in range(len(y))]))\n",
    "\n",
    "# Form and solve problem.\n",
    "prob = cp.Problem(obj)\n",
    "prob.solve()  # Returns the optimal value.\n",
    "print(\"status:\", prob.status)\n",
    "print(\"optimal value\", prob.value)\n",
    "print(\"optimal var\", w.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25446171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2505453990840165, 0.08201220206247475, 0.020031235087500012, 0.0013178370047732036, 3.43049483319885e-06], [0.2540606598125504, 0.09165519373544956, 0.029039781597780936, 0.006055224879481706], [0.25440612039766985, 0.09242721052738719, 0.029683627704031927]]\n",
      "gamma 0.01 eta 0.01 objective 0.2505453990840165\n",
      "gamma 0.01 eta 0.1 objective 0.08201220206247475\n",
      "gamma 0.01 eta 1 objective 0.020031235087500012\n",
      "gamma 0.01 eta 10 objective 0.0013178370047732036\n",
      "gamma 0.01 eta 100 objective 3.43049483319885e-06\n",
      "gamma 0.1 eta 0.01 objective 0.2540606598125504\n",
      "gamma 0.1 eta 0.1 objective 0.09165519373544956\n",
      "gamma 0.1 eta 1 objective 0.029039781597780936\n",
      "gamma 0.1 eta 10 objective 0.006055224879481706\n",
      "gamma 1 eta 0.01 objective 0.25440612039766985\n",
      "gamma 1 eta 0.1 objective 0.09242721052738719\n",
      "gamma 1 eta 1 objective 0.029683627704031927\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def momentum_update(learning_rate, gamma, old_g, w, Y, X):\n",
    "    full_gradient = []\n",
    "    for i in range(X.shape[0]):\n",
    "        gradient = (1/(1+np.exp((Y[i] * (X[i] @ w))))) * (X[i] * -Y[i]) \n",
    "        full_gradient.append(gradient)\n",
    "    gradient = np.sum(full_gradient, axis=0) /X.shape[0]\n",
    "    gradient = np.reshape(gradient, (len(gradient), 1))\n",
    "    g_t = (1- gamma)*old_g + gamma*gradient\n",
    "    #w = w - learning_rate*g_t(gamma, old_g, w, Y, X)\n",
    "    w = w- learning_rate*g_t\n",
    "    assert w.shape == (100, 1), \"w's shape has been changed\"\n",
    "    #return w, g_t(gamma, old_g, w, Y, X)\n",
    "    return w, g_t\n",
    "                  \n",
    "def objective(Y, X, w):\n",
    "    # another try-except here?\n",
    "    np.seterr(over='ignore')\n",
    "    obj = (1/1000)*np.sum(np.log(1+np.exp(np.multiply(-Y, X@w))))\n",
    "    return obj\n",
    "\n",
    "def calc_diff(w, X, Y):\n",
    "    return np.log(objective(Y, X, w) - 2.906898597653035e-06)\n",
    "\n",
    "def mgd(gamma, learning_rate, X, Y):    \n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    old_g = np.zeros((X.shape[1], 1))\n",
    "    diffs = []\n",
    "    for i in range(1000):\n",
    "#         x, y = X[i], Y[i]\n",
    "#         x = np.reshape(x, (1, len(x)))\n",
    "#         y = np.reshape(y, (1, len(y)))\n",
    "        w, old_g = momentum_update(learning_rate, gamma, old_g, w, Y, X)\n",
    "#         if i % 100 == 0:\n",
    "#             print(max(abs(X@w)))\n",
    "        diffs.append(calc_diff(w, X, Y))\n",
    "    return (diffs, objective(Y, X, w))\n",
    "\n",
    "def collect_runs(X, Y):\n",
    "    gammas = [0.01, 0.1, 1]\n",
    "    etas = [0.01, 0.1, 1, 10, 100]\n",
    "    all_diffs, all_obj = [], []\n",
    "    for j in range(len(gammas)):\n",
    "        gamma_diffs = []\n",
    "        objs = []\n",
    "        for i in range(len(etas)):\n",
    "            if j == 0:\n",
    "                diffs, objective = mgd(gammas[j], etas[i], X, Y)\n",
    "                gamma_diffs.append(diffs), objs.append(objective)\n",
    "            elif j ==1 and i != 4:\n",
    "                diffs, objective = mgd(gammas[j], etas[i], X, Y)\n",
    "                gamma_diffs.append(diffs), objs.append(objective)\n",
    "            elif j ==2 and i < 3:\n",
    "                diffs, objective = mgd(gammas[j], etas[i], X, Y)\n",
    "                gamma_diffs.append(diffs), objs.append(objective)\n",
    "            else:\n",
    "                pass\n",
    "        all_obj.append(objs)\n",
    "        #print(j, i, len(gamma_diffs), len(gamma_diffs[0]))\n",
    "        all_diffs.append(gamma_diffs)\n",
    "    return all_diffs, all_obj\n",
    "\n",
    "def graph_em(all_diffs, all_obj):\n",
    "    gammas = [0.01, 0.1, 1]\n",
    "    etas = [0.01, 0.1, 1, 10, 100]\n",
    "    time = [i for i in range(1000)]\n",
    "    print(all_obj)\n",
    "    for i in range(len(all_diffs)):\n",
    "        for j in range(len(all_diffs[i])):\n",
    "            plt.plot(time, all_diffs[i][j], label=\"eta=\"+str(etas[j]))\n",
    "            print(\"gamma\", gammas[i], \"eta\", etas[j], \"objective\", all_obj[i][j])\n",
    "        plt.title(\"gamma=\" + str(gammas[i]) ) \n",
    "        plt.xlabel(\"t\")\n",
    "        plt.ylabel(\"log distance between f(w_t) and f_star\")                    \n",
    "        plt.legend()\n",
    "        plt.savefig(\"log_dist_for_gamma_\"+ str(gammas[i])+\".png\")\n",
    "        plt.close()\n",
    "    \n",
    "X = samples[:, :-1]\n",
    "Y = np.reshape(samples[:, -1], (len(samples[:, -1]), 1))\n",
    "all_diffs, all_obj = collect_runs(X, Y)\n",
    "graph_em(all_diffs, all_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c994ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba4a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 1 eta 1 objective 4.496179268165029\n",
      "batch_size 1 eta 0.3 objective 0.9877771065037633\n",
      "batch_size 1 eta 0.1 objective 0.35739901560532156\n",
      "batch_size 1 eta 0.03 objective 0.25054809882619455\n",
      "batch_size 10 eta 1 objective 0.08956013241793022\n",
      "batch_size 10 eta 0.3 objective 0.0875961519804719\n",
      "batch_size 10 eta 0.1 objective 0.13308225622946246\n",
      "batch_size 10 eta 0.03 objective 0.2188798004880944\n",
      "batch_size 100 eta 1 objective 0.044257433829919396\n",
      "batch_size 100 eta 0.3 objective 0.07702514516162857\n",
      "batch_size 100 eta 0.1 objective 0.12727941895043532\n",
      "batch_size 100 eta 0.03 objective 0.21496493052549362\n",
      "batch_size 1000 eta 1 objective 0.042813358211889724\n",
      "batch_size 1000 eta 0.3 objective 0.07657261420817076\n",
      "batch_size 1000 eta 0.1 objective 0.12673429127223354\n",
      "batch_size 1000 eta 0.03 objective 0.21450404473198176\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def stochastic_update(learning_rate, batch_size, w, data):\n",
    "    #     print((learning_rate*g_t(gamma, old_g, w, y, x)).shape, \"update\")\n",
    "    # need to sample a batch_size of points from X\n",
    "    sample = data[np.random.randint(data.shape[0], size=batch_size), :]\n",
    "    X = sample[:, :-1]\n",
    "    Y = np.reshape(sample[:, -1], (len(sample[:, -1]), 1))\n",
    "\n",
    "    full_gradient = []\n",
    "    for i in range(X.shape[0]):\n",
    "        gradient = (1/(1+np.exp((Y[i] * (X[i] @ w))))) * (-X[i] * Y[i])\n",
    "        full_gradient.append(gradient)\n",
    "    gradient = np.sum(full_gradient, axis=0)* (1/X.shape[0])\n",
    "    gradient = np.reshape(gradient, (len(gradient), 1))\n",
    "    w = w - learning_rate*gradient\n",
    "    assert w.shape == (100, 1), \"w's shape has been changed\"\n",
    "    assert X.shape == (batch_size, 100), \"x's shape has been changed\"\n",
    "    assert Y.shape == (batch_size, 1), \"y's shape has been changed\"\n",
    "    return w\n",
    "\n",
    "                  \n",
    "def objective(data, w):\n",
    "    X = data[:, :-1]\n",
    "    Y = np.reshape(data[:, -1], (len(data[:, -1]), 1))\n",
    "    np.seterr(over='ignore')\n",
    "    #print(X.shape, Y.shape, w.shape)\n",
    "    obj = (1/X.shape[0])*np.sum(np.log(1+np.exp(np.multiply(-Y, X@w))))\n",
    "    return obj\n",
    "\n",
    "# what am I supposed to do with the vals\n",
    "def sgd(batch_size, learning_rate, data):    \n",
    "    \n",
    "    diffs = []\n",
    "    for epoch in range(25):\n",
    "        epoch_diffs = []\n",
    "        w = np.zeros((X.shape[1], 1))\n",
    "        for i in range(500):\n",
    "            w = stochastic_update(learning_rate, batch_size, w, data)\n",
    "            epoch_diffs.append(calc_diff(w, data))\n",
    "        diffs.append(epoch_diffs)\n",
    "    last_obj = objective(data, w)\n",
    "    return diffs, last_obj\n",
    "\n",
    "\n",
    "def calc_diff(w, data):\n",
    "    return np.log(objective(data, w) - 2.906898597653035e-06)\n",
    "\n",
    "def collect_runs(data):\n",
    "    batch_sizes = [1,10,100,1000]\n",
    "    etas = [1, 0.3, 0.1, 0.03]\n",
    "    all_avg_diffs = []\n",
    "    for j in range(len(batch_sizes)):\n",
    "        batch_diffs = []\n",
    "        for i in range(len(etas)):\n",
    "            diffs, last_obj = sgd(batch_sizes[j], etas[i], data)\n",
    "            print(\"batch_size\", batch_sizes[j], \"eta\", etas[i], \"objective\", last_obj)\n",
    "            diffs = np.array(diffs)\n",
    "            # get the avg\n",
    "            #print(diffs.shape)\n",
    "            all_var = np.mean(diffs, axis=0)\n",
    "            #print(all_var.shape)\n",
    "            batch_diffs.append(all_var)\n",
    "        all_avg_diffs.append(batch_diffs)\n",
    "    return all_avg_diffs\n",
    "\n",
    "def graph_em(all_avg_objs):\n",
    "    batch_sizes = [1,10,100,1000]\n",
    "    etas = [1, 0.3, 0.1, 0.03]\n",
    "    time = [i for i in range(500)]\n",
    "    for i in range(len(all_avg_objs)):\n",
    "        for j in range(len(all_avg_objs[i])):\n",
    "            # this will need more advanced logic\n",
    "            plt.plot(time, all_avg_objs[i][j], label=\"eta=\"+str(etas[j]))\n",
    "        plt.title(\"batch_size=\" + str(batch_sizes[i])) \n",
    "        plt.xlabel(\"t\")\n",
    "        plt.ylabel(\"log distance between f(w_t) and f_star\")                    \n",
    "        plt.legend()\n",
    "        plt.savefig(\"log_dist_for_batch_size_\"+ str(batch_sizes[i])+\".png\")\n",
    "        plt.close()\n",
    "    # \n",
    "    for j in range(len(all_avg_objs[0])):\n",
    "        for i in range(len(all_avg_objs)):\n",
    "            # this will need more advanced logic\n",
    "            plt.plot(time, all_avg_objs[i][j], label=\"batch_size=\" + str(batch_sizes[i]))\n",
    "        plt.title(\"eta=\"+str(etas[j])) \n",
    "        plt.xlabel(\"t\")\n",
    "        plt.ylabel(\"log distance between f(w_t) and f_star\")                    \n",
    "        plt.legend()\n",
    "        plt.savefig(\"log_dist_for_learning_rate_\"+ str(etas[j])+\".png\")\n",
    "        plt.close()\n",
    "    # will need a second set of graphs\n",
    "                   \n",
    "    \n",
    "all_obj= collect_runs(samples)\n",
    "graph_em(all_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecfd19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
