{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7605b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "csv = np.genfromtxt ('train.csv', delimiter=\",\")\n",
    "test_csv = np.genfromtxt ('test.csv', delimiter=\",\")\n",
    "# each row is 784 + 1 cols separated by commas\n",
    "# column 785 contains the corresponding label (0 to 9), last col is y_vect\n",
    "train_x, train_y = csv[:,:-1], csv[:,-1]\n",
    "test_x, test_y = test_csv[:,:-1], test_csv[:,-1]\n",
    "# import params\n",
    "alpha = np.genfromtxt ('alpha1.txt', delimiter=\",\")\n",
    "beta = np.genfromtxt ('alpha2.txt', delimiter=\",\")\n",
    "bias1 = np.genfromtxt ('beta1.txt', delimiter=\",\")\n",
    "bias2 = np.genfromtxt ('beta2.txt', delimiter=\",\")\n",
    "alpha = np.concatenate((bias1.reshape(bias1.shape[0], 1), alpha), axis=1)\n",
    "beta = np.concatenate((bias2.reshape(bias2.shape[0], 1), beta), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcc5f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 2., 9., ..., 7., 9., 2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y\n",
    "# z = 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbcfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer1:\n",
    "    def __init__(self,x_vect,alpha):\n",
    "        # add x_0 to x for the forward pass\n",
    "        #self.x_star = x_vect.T\n",
    "        self.x_vect = x_vect.T\n",
    "        self.x_vect = np.insert(self.x_vect, 0, 1, axis=0)\n",
    "        #print(self.x_star.shape, \"x_star\")\n",
    "        #print(self.x_vect.shape, \"x_vect\")\n",
    "        # add col to beg of a of all 1s\n",
    "        # concat alpha and beta together, beta is bias\n",
    "        self.alpha_star = alpha[:,1:]\n",
    "        #self.alpha = np.concatenate((bias.reshape(bias.shape[0], 1), alpha), axis=1)\n",
    "        self.alpha = alpha\n",
    "        #print(self.alpha.shape, \"alpha\")\n",
    "\n",
    "    def forward(self):\n",
    "        #print(self.x_star.shape,self.alpha.shape, self.x_vect.shape)\n",
    "        \n",
    "        #print((np.dot(self.alpha, self.x_vect)).shape)\n",
    "        return np.dot(self.alpha, self.x_vect)\n",
    "    def backward(self, passed_in):\n",
    "        # dl/d_alpha= dl/db * db/dz * dz/da * da/d_alpha\n",
    "        # all but da/d_alpha will get passed in\n",
    "        return np.dot(passed_in, self.x_vect.reshape(1, self.x_vect.shape[0]))\n",
    "        #return np.dot(passed_in, self.x_star.reshape(1, self.x_star.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf936c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer:\n",
    "    def __init__(self,A):\n",
    "        #print(A.shape)\n",
    "        self.A = A  \n",
    "    def forward(self):\n",
    "        return 1/(1 + np.exp(-self.A))\n",
    "    def backward(self, passed_in):\n",
    "        # dl/d_a= dl/db * db/dz * dz/da \n",
    "        sigmoid = 1/(1 + np.exp(-self.A))\n",
    "        # all but dz/da will get passed in\n",
    "        #dz_da = np.exp(-self.A) * (1+ np.exp(-self.A))**(-2)\n",
    "        return np.array([sigmoid[i]*(1-sigmoid[i]) for i in range(passed_in.shape[0])])\n",
    "        #return passed_in *dz_da[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0116594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer2:\n",
    "    def __init__(self,Z,beta):\n",
    "        # add 1 for z_0\n",
    "        self.Z = Z\n",
    "        self.Z = np.insert(self.Z, 0, 1, axis=0)\n",
    "        self.beta_star = beta[:,1:]\n",
    "        self.beta = beta \n",
    "    def forward(self):\n",
    "        \n",
    "        return np.dot(self.beta, self.Z)\n",
    "    def backward(self, passed_in):\n",
    "        # dl/dz= dl/db * db/dz\n",
    "        # all but db/dz will get passed in\n",
    "        #print(passed_in.shape, self.beta_star.T.shape, self.Z.shape)\n",
    "        return np.dot(self.beta_star.T, passed_in)\n",
    "    def beta_gradient(self, passed_in):\n",
    "        # dl/dz= dl/db * db/dbeta\n",
    "        # all but db/dbeta will get passed in\n",
    "        return np.dot(passed_in.reshape(passed_in.shape[0], 1), self.Z.reshape(1, self.Z.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf58b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'return np.sum(np.sum(np.dot(self.y,                                     (self.B -                                     np.log(np.sum(np.exp(self.B), axis =0))).                                     reshape(self.B.shape[1],self.B.shape[0] ))))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_y_vect(y_vect, B):\n",
    "    # translate the int at each element to a np array with a 1 at that index, zero ow\n",
    "    # init matrix of zeros, directly use y_vect to index the matrix\n",
    "    if len(y_vect.shape) != 1:\n",
    "        return y_vect\n",
    "    y = np.zeros((B.shape[0], y_vect.shape[0]))\n",
    "    #print(y.shape)\n",
    "    for row_index in range((y_vect.shape[0])):\n",
    "        #print(y_vect[row_index])\n",
    "        y[int(y_vect[row_index]), row_index] = 1\n",
    "    return y\n",
    "\n",
    "\"\"\"return np.sum(np.sum(np.dot(self.y, \\\n",
    "                                    (self.B - \\\n",
    "                                    np.log(np.sum(np.exp(self.B), axis =0))). \\\n",
    "                                    reshape(self.B.shape[1],self.B.shape[0] ))))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f9ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCELayer:\n",
    "    def __init__(self,B, y_vect):\n",
    "        self.B = B\n",
    "        #print(self.B.shape)\n",
    "        self.y_hat_vect = np.exp(self.B)/ np.sum(np.exp(self.B), axis =0)\n",
    "        #print(self.y_hat_vect.shape)\n",
    "        self.y = translate_y_vect(y_vect, self.B)\n",
    "        # was [0] before\n",
    "        self.N = self.y.shape[0]\n",
    "        #print(self.y_hat_vect.shape, self.y.shape, \"y_hat, y\")\n",
    "    def forward(self):\n",
    "        return sum(-sum([self.y[i] * np.log(self.y_hat_vect[i]) for i in range(self.y.shape[0])]))\n",
    "        #np.sum(np.sum(np.dot(self.y, np.log((self.y_hat_vect.reshape(self.y.shape[1], self.y.shape[0],))))))\n",
    "\n",
    "    def backward(self):\n",
    "        # dl/db\n",
    "        #sum( self.y_hat_vect[i] - self.y[i]for i in range(self.y.shape[0])])\n",
    "        #print((-1/self.N)*np.sum(np.subtract(self.y_hat_vect, self.y), axis=1))\n",
    "        return (-1/self.N)*np.sum(np.subtract(self.y_hat_vect, self.y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518518c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self,X, y, alpha, beta):\n",
    "        self.whole_X =X\n",
    "        self.whole_y =y\n",
    "        self.batch_x = 0\n",
    "        self.batch_y = 0\n",
    "        self.alpha =alpha\n",
    "        self.beta =beta\n",
    "        self.loss =  0\n",
    "        self.dl_db = 0\n",
    "        self.dl_dbeta = 0\n",
    "        self.dl_dalpha = 0\n",
    "        self.a = 0\n",
    "        self.z = 0\n",
    "        self.B = 0\n",
    "        self.y_hat = 0\n",
    "    def forward_pass(self):\n",
    "        self.a = LinearLayer1(self.batch_x, self.alpha).forward()\n",
    "        self.alpha = LinearLayer1(self.batch_x, self.alpha).alpha\n",
    "        self.z = SigmoidLayer(self.a).forward()\n",
    "        #print(self.z.shape)\n",
    "        self.B = LinearLayer2(self.z, self.beta).forward()\n",
    "        self.beta = LinearLayer2(self.z,self.beta).beta\n",
    "        #print(self.a.shape, self.z.shape, self.B.shape)\n",
    "        self.y_hat = SoftmaxCELayer(self.B, self.batch_y).y_hat_vect\n",
    "        #print(translate_y_vect(self.batch_y, np.zeros((10,1))).shape, self.y_hat.shape)\n",
    "        self.loss = SoftmaxCELayer(self.B, self.batch_y).forward()\n",
    "            \n",
    "    def backward_pass(self):\n",
    "        self.dl_db = SoftmaxCELayer(self.B, self.batch_y).backward()\n",
    "        #print(self.dl_db.shape, \"dl_db\")\n",
    "        dl_dz = LinearLayer2(self.z,self.beta).backward(self.dl_db)\n",
    "        #print(dl_dz.shape, \"dl_dz\")\n",
    "        self.dl_dbeta = LinearLayer2(self.z,self.beta).beta_gradient(self.dl_db)\n",
    "        #print(self.dl_db.shape, \"dl_dbeta\")\n",
    "        dl_da = SigmoidLayer(self.a).backward(dl_dz)\n",
    "        #print(dl_da.shape, \"dl_da\")\n",
    "        self.dl_dalpha = LinearLayer1(self.batch_x, self.alpha).backward(dl_da)\n",
    "        #print(self.dl_dalpha.shape, \"dl_dalpha\")\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.alpha = self.alpha - learning_rate*self.dl_dalpha\n",
    "        self.beta = self.beta - learning_rate*self.dl_dbeta\n",
    "            \n",
    "    def get_test_loss(self, test_x, test_y):\n",
    "        self.batch_x = test_x\n",
    "        #print(test_y.shape, \"before trans\")\n",
    "        self.batch_y = translate_y_vect(test_y, np.zeros((10,1)))\n",
    "        #print(self.batch_y.shape, \"after trans\")\n",
    "        self.forward_pass()\n",
    "        #print(self.y_hat)\n",
    "        return self.loss\n",
    "    \n",
    "    def test_acccuracy(self, test_y):\n",
    "        #print(test_y.shape, \"before trans\", self.y_hat.shape)\n",
    "        self.batch_y = translate_y_vect(test_y, np.zeros((10,1)))\n",
    "        #print(self.batch_y.shape, \"after trans\", self.y_hat.shape)\n",
    "        correct_prediction = 0\n",
    "        incorrect_prediction = 0\n",
    "        # correct prediction\n",
    "        #(10, 1000) after trans (10, 1000)\n",
    "        for row in range((self.batch_y.shape[1])):\n",
    "            A = (self.batch_y[:,row]- self.y_hat[:,row])\n",
    "            B = np.all(np.zeros((10,1)))\n",
    "            if (A==B).all():\n",
    "                correct_prediction += 1\n",
    "            else:\n",
    "                incorrect_prediction += 1\n",
    "        # correct / correct+incorrect\n",
    "        return correct_prediction/ (correct_prediction+ incorrect_prediction)\n",
    "            \n",
    "    def train(self, test_x, test_y, learning_rate, epochs, batch_size):\n",
    "        loss_list = []\n",
    "        test_list = []\n",
    "        accuracy_list = []\n",
    "        # modify alpha and beta from beginning\n",
    "        for epoch in range(epochs):\n",
    "            for batch_index in range(int(len(self.whole_X)/batch_size)):\n",
    "                \n",
    "                self.batch_x = self.whole_X[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "                self.batch_y = self.whole_y[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "                #print(self.alpha.shape, self.beta.shape, self.loss)\n",
    "                #print(self.batch_x.shape,self.batch_y.shape)\n",
    "                self.forward_pass()\n",
    "                #print(self.alpha.shape, self.beta.shape, self.loss)\n",
    "                \n",
    "                #print(self.y_hat)\n",
    "                self.backward_pass()\n",
    "                #print(self.alpha.shape, self.beta.shape, self.loss)\n",
    "                self.update(learning_rate)\n",
    "                #print(self.beta)\n",
    "                #print(self.alpha)\n",
    "            if epoch == 2:\n",
    "                print(self.beta[:,0])\n",
    "            # compute training loss for epoch\n",
    "            loss_list.append(np.array(self.loss))\n",
    "            # compute test loss\n",
    "            (test_list.append(np.array(self.get_test_loss(test_x, test_y))))\n",
    "            # compute test accuracy\n",
    "            (accuracy_list.append(np.array(self.test_acccuracy(test_y))))\n",
    "        return loss_list, test_list, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077c2b2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.80039478 -1.52145295 -1.69371438 -0.34914953 -0.82852895  7.58397735\n",
      " -1.3999061  -0.7435914   1.18845782 -0.45260243]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 1\n",
    "nn = Network(train_x, train_y, alpha, beta)\n",
    "loss_list, test_list, accuracy_list = nn.train(test_x, test_y, learning_rate, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61946800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.),\n",
       " array(0.)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed90af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(8157.77111597),\n",
       " array(14639.89443956),\n",
       " array(7840.27042446),\n",
       " array(10362.07394332),\n",
       " array(13040.53663109),\n",
       " array(15729.48857137),\n",
       " array(18422.42304541),\n",
       " array(21117.43142475),\n",
       " array(23813.70331287),\n",
       " array(26510.78661003),\n",
       " array(29208.4004221),\n",
       " array(31906.38258513),\n",
       " array(34604.63718856),\n",
       " array(37303.10651587),\n",
       " array(40001.75510842)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad4aa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nlearning_rate = 0.01\\nepochs = 100\\nbatch_size = 1\\nnn = Network(train_x, train_y, alpha, bias1, beta, bias2)\\nloss_list, test_list, accuracy_list = nn.train(test_x, test_y, learning_rate, epochs, batch_size)\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "nn = Network(train_x, train_y, alpha, bias1, beta, bias2)\n",
    "loss_list, test_list, accuracy_list = nn.train(test_x, test_y, learning_rate, epochs, batch_size)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1eab9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlearning_rate = 0.01\\nepochs = 100\\nbatch_size_ls = [1, 10, 50, 100]\\nloss_ls_ls = np.array([])\\ntest_loss_ls_ls = np.array([])\\naccuracy_ls_ls = np.array([])\\nfor batch_size in batch_size_ls:\\n    nn = Network(train_x, train_y, alpha, bias1, beta, bias2)\\n    loss_list, test_list, accuracy_list = nn.train(test_x, test_y, learning_rate, epochs, batch_size)\\n    np.vstack(loss_ls_ls, loss_list)\\n    np.vstack(test_loss_ls_ls, test_list)\\n    np.vstack(accuracy_ls_ls, accuracy_list)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size_ls = [1, 10, 50, 100]\n",
    "loss_ls_ls = np.array([])\n",
    "test_loss_ls_ls = np.array([])\n",
    "accuracy_ls_ls = np.array([])\n",
    "for batch_size in batch_size_ls:\n",
    "    nn = Network(train_x, train_y, alpha, bias1, beta, bias2)\n",
    "    loss_list, test_list, accuracy_list = nn.train(test_x, test_y, learning_rate, epochs, batch_size)\n",
    "    np.vstack(loss_ls_ls, loss_list)\n",
    "    np.vstack(test_loss_ls_ls, test_list)\n",
    "    np.vstack(accuracy_ls_ls, accuracy_list)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env-kernel",
   "language": "python",
   "name": "jupyter-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
